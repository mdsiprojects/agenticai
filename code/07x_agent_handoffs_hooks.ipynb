{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe52035e",
   "metadata": {},
   "source": [
    "https://github.com/openai/openai-agents-python/blob/main/examples/basic/agent_lifecycle_example.py \n",
    "\n",
    "two agents\n",
    "each has its own tools \n",
    "\n",
    "1. starter agents: will process the initial user request, will use tools as needed and will handoff to other agents if required\n",
    "2. multiply agent: knows how to perform arithmetic operations (can we configure a handoff)\n",
    "\n",
    "example also keeps tracks of agent life cycle events \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f0c45d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from agents import (\n",
    "\n",
    "    Agent,\n",
    "    Runner,\n",
    "    RunConfig,\n",
    "    trace,\n",
    "    ModelSettings,\n",
    "    AgentHooks,\n",
    "    RunHooks,\n",
    "    TContext,\n",
    "    MaxTurnsExceeded,\n",
    "    Tool\n",
    ")\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467e7783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, RawResponsesStreamEvent, Runner, TResponseInputItem, trace\n",
    "from helpers.trace_util import get_trace_url\n",
    "from agents.extensions.visualization import draw_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c00e1ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.model_client import (\n",
    "    get_openai_client,\n",
    "    get_github_model_provider\n",
    ")\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "GITHUB_MODEL_PROVIDER = get_github_model_provider(\n",
    "    client = get_openai_client(),\n",
    "    model = model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1c7ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n",
    "from agents import handoff, RunContextWrapper\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str = Field(description=\"The joke text\")\n",
    "    punchline: str = Field(description=\"The punchline of the joke\")\n",
    "    language: str = Field(description=\"The language of the joke\")\n",
    "\n",
    "\n",
    "class Translation(BaseModel):\n",
    "    content: str = Field(description=\"Content\")\n",
    "    language: str = Field(description=\"The language of the translation\")\n",
    "\n",
    "async def on_handoff1(ctx: RunContextWrapper[None], input_data: Translation):\n",
    "    print(f\"agent {ctx} called with: {input_data.content}\")\n",
    "\n",
    "dad_jokes_agent = Agent(\n",
    "    name=\"dad_jokes_generator_agent\",\n",
    "    instructions=(\n",
    "        # f\"{RECOMMENDED_PROMPT_PREFIX} \\n\"\n",
    "        \"You are an agent that specialises in generating jokes, dad jokes specifically. \"\n",
    "        \"You only respond with dad jokes.\"\n",
    "        \"You do not answer any other questions. \"\n",
    "        \"always deliver a punchline. \"\n",
    "        \" ## IMPORTANT ##\"\n",
    "        \"Always respond in English, even if the user asks in a different language. \"\n",
    "        \"You don't end the conversation, you must handoff to another agent.\"\n",
    "    ),\n",
    "    handoff_description=(\n",
    "        \"An agent capable of generating jokes in English\"\n",
    "    ),\n",
    "    model=model_name,\n",
    "    output_type=Joke,\n",
    ")\n",
    "\n",
    "\n",
    "spanish_agent = Agent(\n",
    "    name=\"spanish_agent\",\n",
    "    instructions= \"\"\"\n",
    "        You translate the input message to Spanish.\n",
    "        You don't generate new content.\n",
    "        ## IMPORTANT ##\n",
    "        You don't end the conversation, you must handoff to another agent.\n",
    "        only handoff to a single agent at a time\n",
    "\n",
    "        \"\"\",\n",
    "    handoff_description=\"\"\"\n",
    "    You translate content to spanish.\n",
    "    \"\"\",\n",
    "    model=model_name,\n",
    "    output_type=Translation,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "french_agent = Agent(\n",
    "    name=\"french_agent\",\n",
    "    instructions= \"\"\"\n",
    "        You translate the input message to French.\n",
    "        You don't generate new content.\n",
    "        ## IMPORTANT ##\n",
    "        You don't end the conversation, you must handoff to another agent.\n",
    "        only handoff to a single agent at a time\n",
    "\n",
    "\n",
    "        \"\"\",\n",
    "    handoff_description=\"you translate content to french\",\n",
    "    model=model_name,\n",
    "    output_type=Translation,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e336f",
   "metadata": {},
   "source": [
    "# Deterministic Workflow: Manual Orchestration\n",
    "\n",
    "In this example, we will execute a manual handoff between agents in a sequential workflow. \n",
    "\n",
    "All of the agents produce structured outputs. \n",
    "\n",
    "The output of one agent is used as the input to the next agent, after appending to it an additional handoff instruction message.\n",
    "\n",
    "Notice in the trace afterwards how the full message history is preserved, including the handoff instruction message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d568294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def handoff_instructions_msg(lang: str):\n",
    "    return {\"role\": \"user\", \"content\": f\"Translate the following text to {lang}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a8fb4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace URL: https://platform.openai.com/traces/trace?trace_id=trace_379e8e6be1ff407898f0c31753bc57df\n",
      "RunResult:\n",
      "- Last agent: Agent(name=\"dad_jokes_generator_agent\", ...)\n",
      "- Final output (Joke):\n",
      "    {\n",
      "      \"joke\": \"Why can't you give Elsa a balloon?\",\n",
      "      \"punchline\": \"Because she will let it go!\",\n",
      "      \"language\": \"English\"\n",
      "    }\n",
      "- 1 new item(s)\n",
      "- 1 raw response(s)\n",
      "- 0 input guardrail result(s)\n",
      "- 0 output guardrail result(s)\n",
      "(See `RunResult` for more details)\n",
      "First joke as Joke: joke=\"Why can't you give Elsa a balloon?\" punchline='Because she will let it go!' language='English'\n",
      "Spanish translation as Joke: joke='¿Por qué no puedes darle un globo a Elsa?' punchline='¡Porque lo dejará ir!' language='Spanish'\n",
      "French translation as Joke: joke='Pourquoi ne peux-tu pas donner un ballon à Elsa ?' punchline=\"Parce qu'elle va le laisser partir !\" language='French'\n",
      "joke='Pourquoi ne peux-tu pas donner un ballon à Elsa ?' punchline=\"Parce qu'elle va le laisser partir !\" language='French'\n"
     ]
    }
   ],
   "source": [
    "input_list = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a dad joke\"},\n",
    "]\n",
    "\n",
    "\n",
    "with trace(\"dad_jokes_agent_manual_handoff\", group_id=\"dad_jokes\") as tr:\n",
    "\n",
    "    print(f\"Trace URL: {get_trace_url(tr)}\")\n",
    "\n",
    "    run_output = await Runner.run(\n",
    "        starting_agent= dad_jokes_agent,\n",
    "        input=input_list,\n",
    "        run_config = RunConfig(\n",
    "            model_provider=GITHUB_MODEL_PROVIDER,\n",
    "            model = model_name,\n",
    "        ),\n",
    "        )\n",
    "\n",
    "\n",
    "    print(run_output)\n",
    "\n",
    "    # joke_output : Joke = run_output.final_output\n",
    "    # print(f\"Joke: {joke_output.joke}\")\n",
    "    print(f\"First joke as Joke: {run_output.final_output_as(Joke)}\")\n",
    "\n",
    "\n",
    "    next_input = run_output.to_input_list()\n",
    "    next_input.append(handoff_instructions_msg(\"Spanish\"))\n",
    "\n",
    "    spanish_agent.output_type = Joke\n",
    "    spanish_output = await Runner.run(\n",
    "        starting_agent=spanish_agent,\n",
    "        input=next_input,\n",
    "        run_config = RunConfig(\n",
    "            model_provider=GITHUB_MODEL_PROVIDER,\n",
    "            model = model_name,\n",
    "        ),\n",
    "        )\n",
    "\n",
    "    print(f\"Spanish translation as Joke: {spanish_output.final_output_as(Joke)}\")\n",
    "\n",
    "\n",
    "    french_agent.output_type = Joke\n",
    "    french_agent_input = spanish_output.to_input_list()\n",
    "    french_agent_input.append(handoff_instructions_msg(\"French\"))\n",
    "    french_output = await Runner.run(\n",
    "        starting_agent=french_agent,\n",
    "        input=french_agent_input,\n",
    "        run_config = RunConfig(\n",
    "            model_provider=GITHUB_MODEL_PROVIDER,\n",
    "            model = model_name,\n",
    "        ),\n",
    "        )\n",
    "\n",
    "\n",
    "    print(f\"French translation as Joke: {french_output.final_output_as(Joke)}\")\n",
    "\n",
    "    french_output.to_input_list()\n",
    "\n",
    "\n",
    "print(french_output.final_output_as(Joke))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f7b7d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelResponse(output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='{\"joke\":\"Pourquoi ne peux-tu pas donner un ballon à Elsa ?\",\"punchline\":\"Parce qu\\'elle va le laisser partir !\",\"language\":\"French\"}', type='output_text')], role='assistant', status='completed', type='message')], usage=Usage(requests=1, input_tokens=248, output_tokens=34, total_tokens=282), referenceable_id=None)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Tell me a dad joke'},\n",
       " {'id': '__fake_id__',\n",
       "  'content': [{'annotations': [],\n",
       "    'text': '{\"joke\":\"Why can\\'t you give Elsa a balloon?\",\"punchline\":\"Because she will let it go!\",\"language\":\"English\"}',\n",
       "    'type': 'output_text'}],\n",
       "  'role': 'assistant',\n",
       "  'status': 'completed',\n",
       "  'type': 'message'},\n",
       " {'role': 'user', 'content': 'Translate the following text to Spanish'},\n",
       " {'id': '__fake_id__',\n",
       "  'content': [{'annotations': [],\n",
       "    'text': '{\"joke\":\"¿Por qué no puedes darle un globo a Elsa?\",\"punchline\":\"¡Porque lo dejará ir!\",\"language\":\"Spanish\"}',\n",
       "    'type': 'output_text'}],\n",
       "  'role': 'assistant',\n",
       "  'status': 'completed',\n",
       "  'type': 'message'},\n",
       " {'role': 'user', 'content': 'Translate the following text to French'},\n",
       " {'id': '__fake_id__',\n",
       "  'content': [{'annotations': [],\n",
       "    'text': '{\"joke\":\"Pourquoi ne peux-tu pas donner un ballon à Elsa ?\",\"punchline\":\"Parce qu\\'elle va le laisser partir !\",\"language\":\"French\"}',\n",
       "    'type': 'output_text'}],\n",
       "  'role': 'assistant',\n",
       "  'status': 'completed',\n",
       "  'type': 'message'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(french_output.raw_responses)\n",
    "\n",
    "next_input = french_output.to_input_list()\n",
    "display(next_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67538b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage(requests=1, input_tokens=248, output_tokens=34, total_tokens=282)\n"
     ]
    }
   ],
   "source": [
    "print(french_output.raw_responses[-1].usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b6f34f",
   "metadata": {},
   "source": [
    "# Implement Handoffs\n",
    "\n",
    "In this example, we will implement the handoffs using the SDK handoff functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bd0d0f",
   "metadata": {},
   "source": [
    "## Define the Orchestrator and Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba403cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_response_agent = Agent(\n",
    "    name=\"user_response_agent\",\n",
    "    instructions=\"\"\"\n",
    "        You are an agent that generates a response to the user's message.\n",
    "        your response should include summary of steps taken to produce the final output\n",
    "        in bullet points.\n",
    "        present this execution summary as a list of steps taken to produce the final output.\"\n",
    "        then in the response back to the user, include the final output from each step,\n",
    "        the user query, and the very last final output\"\n",
    "    \"\"\",\n",
    "    handoff_description=\"\"\"\n",
    "    An agent capable of generating final user responses\n",
    "    Final user response is a summary of the steps taken to produce the final output.\n",
    "    \"\"\",\n",
    "    model=model_name,\n",
    "    output_type=str,\n",
    ")\n",
    "\n",
    "\n",
    "orchestrator_agent = Agent(\n",
    "    name=\"orchestrator_agent\",\n",
    "    model=model_name,\n",
    "    tool_use_behavior=\"run_llm_again\",\n",
    "    instructions=\"\"\"\n",
    "        You are an orchestrator agent.\n",
    "        You breakdown the task to multiple steps and handoff to appropriate agents.\n",
    "        You are capable of generating jokes, translating messages to Spanish and French.\n",
    "        If asked for multiple translations, handoff to multiple language agents.\n",
    "        You only speak English. Do not translate the user's message yourself.\n",
    "        Use translator only when you have content to be translated, otherwise, handoff to the next agent.\n",
    "        ## IMPORTANT ##\n",
    "        Structure the execution plan such that you first generate content, for example by using\n",
    "        the joke generator, then translate it using translation agents, and finally,\n",
    "        handoff to the user response agent.\n",
    "        Express your thoughts before executing the next step\n",
    "    \"\"\",\n",
    "    handoff_description=(\n",
    "        \"An agent that orchestrates tasks.\"\n",
    "        ),\n",
    "    # handoffs=[dad_jokes_agent,user_response_agent]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# # orchestrator_agent.handoffs = [dad_jokes_agent,user_response_agent]\n",
    "# dad_jokes_agent.handoffs = [spanish_agent, french_agent, user_response_agent]\n",
    "# spanish_agent.handoffs = [user_response_agent]\n",
    "# french_agent.handoffs = [user_response_agent]\n",
    "\n",
    "dad_jokes_agent.handoffs = []\n",
    "spanish_agent.handoffs = []\n",
    "french_agent.handoffs = []\n",
    "user_response_agent.handoffs = []\n",
    "orchestrator_agent.handoffs = []\n",
    "\n",
    "\n",
    "dad_jokes_agent.handoffs = [\n",
    "    handoff(orchestrator_agent),\n",
    "    # spanish_agent,\n",
    "    # french_agent,\n",
    "    ]\n",
    "\n",
    "# spanish_agent.handoffs = [user_response_agent]\n",
    "# french_agent.handoffs = [user_response_agent]\n",
    "\n",
    "orchestrator_agent.handoffs = [\n",
    "    dad_jokes_agent,\n",
    "    user_response_agent,\n",
    "    ]\n",
    "\n",
    "dad_jokes_agent.tools = [\n",
    "    spanish_agent.as_tool(\n",
    "        tool_name=\"translate_to_spanish\",\n",
    "        tool_description=\"Translate the input message to Spanish\",\n",
    "    ),\n",
    "    french_agent.as_tool(\n",
    "        tool_name=\"translate_to_french\",\n",
    "        tool_description=\"Translate the input message to French\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# # Handoff callback that processes the escalation data\n",
    "async def process_handoff(ctx: RunContextWrapper, input_data: Any):\n",
    "    print(f\"[handoff] Handoff to {ctx.agent.name}\")\n",
    "    print(f\"[handoff] Input data: {input_data}\")\n",
    "    #    print(f\"[handoff] Joke: {input_data.joke}\")\n",
    "    #    print(f\"[handoff] Punchline: {input_data.punchline}\")\n",
    "    #    print(f\"[handoff] Language: {input_data.language}\")\n",
    "\n",
    "\n",
    "\n",
    "# dad_jokes_agent.handoffs = [\n",
    "#    handoff(\n",
    "#          agent=spanish_agent,\n",
    "#          input_type=Joke,\n",
    "#          on_handoff=process_handoff,\n",
    "#    ),\n",
    "#    handoff(\n",
    "#          agent=french_agent,\n",
    "#          input_type=Joke,\n",
    "#          on_handoff=process_handoff,\n",
    "#    ),\n",
    "#     handoff(\n",
    "#             agent=user_response_agent,\n",
    "#             input_type=Joke,\n",
    "#             on_handoff=process_handoff,\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "\n",
    "try:\n",
    "    gv = draw_graph(\n",
    "        orchestrator_agent,\n",
    "        filename=\"./viz/07_agent_handoffs_orchestrator_agent.gv\",\n",
    "\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57151ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "orchestrator_agent.handoffs\n",
    "len(dad_jokes_agent.handoffs)\n",
    "dad_jokes_agent.handoffs[0]\n",
    "\n",
    "print(len(spanish_agent.handoffs))\n",
    "print(len(dad_jokes_agent.handoffs))\n",
    "print(len(french_agent.handoffs))\n",
    "print(len(orchestrator_agent.handoffs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08532cc2",
   "metadata": {},
   "source": [
    "## Add hooks\n",
    "\n",
    "Hooks allow you to receive callbacks on various lifecycle events of the agent or the run. \n",
    "\n",
    "This in turn allows you to implement custom logic, such as logging, error handling, or modifying the agent's behavior based on certain conditions.\n",
    "\n",
    "You can manipulate context and state in the hooks. \n",
    "\n",
    "\n",
    "The hook events are as follows:\n",
    "| Event | Agents | Runs | \n",
    "|--------|-------|-------------|\n",
    "| Start | `on_agent_start` | `on_start` | \n",
    "| Handoff | `on_handoff` | `on_handoff` |\n",
    "| Tool Start | `on_tool_start` | `on_tool_start` |\n",
    "| Tool End | `on_tool_end` | `on_tool_end` |\n",
    "| End | `on_agent_end` | `on_end` |\n",
    "\n",
    "To implement hooks you need to subclass `AgentHooks` or `RunHooks` and implement the methods you want to use, https://openai.github.io/openai-agents-python/ref/lifecycle/#agents.lifecycle.AgentHooks. \n",
    "\n",
    "After you implemented the hooks, you can then attach them to an agent or run using the `hooks` attribute or parameter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29e536eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokeWorkflowRunHook(RunHooks):\n",
    "    def __init__(\n",
    "            self,\n",
    "            custom_state_object: Optional[Dict[str, Any]]):\n",
    "        super().__init__()\n",
    "        self.custom_state : Dict[str,any] = custom_state_object\n",
    "\n",
    "    def on_handoff(\n",
    "            self,\n",
    "            context: RunContextWrapper[TContext],\n",
    "            from_agent: Agent[TContext],\n",
    "            to_agent: Agent[TContext]) -> None:\n",
    "\n",
    "        print(f\"[handoff] Handoff context: {context}\")\n",
    "        print(f\"[handoff] Handoff from: {from_agent.name}\")\n",
    "        print(f\"[handoff] Handoff to: {to_agent.name}\")\n",
    "\n",
    "        # increment state object\n",
    "        handoff_count = self.custom_state.get(\"handoffs_count\", 0) + 1\n",
    "        print(f\"[handoff] HandoffsCount: {handoff_count}\")\n",
    "\n",
    "        # you can use hooks to raise exceptions to interrupt the workflow\n",
    "        max_handoffs_count = self.custom_state.get(\"max_handoffs_count\", 100)\n",
    "        if handoff_count > max_handoffs_count:\n",
    "            raise MaxTurnsExceeded(\"Max handoffs count reached\")\n",
    "\n",
    "        # always pass on the event\n",
    "        return super().on_handoff(context, from_agent, to_agent)\n",
    "\n",
    "    def on_agent_end(\n",
    "            self,\n",
    "            context: RunContextWrapper[TContext],\n",
    "            agent: Agent[TContext],\n",
    "            output: Any) -> None:\n",
    "        print(f\"[on_agent_end] Agent: {agent.name}\")\n",
    "        print(f\"[on_agent_end] Context: {context}\")\n",
    "        print(f\"[on_agent_end] Output: {output}\")\n",
    "\n",
    "        return super().on_agent_end(context, agent, output)\n",
    "\n",
    "\n",
    "    def on_agent_start(self, context, agent):\n",
    "        print(f\"[on_agent_start] Agent: {agent.name}\")\n",
    "        print(f\"[on_agent_start] Context: {context}\")\n",
    "        return super().on_agent_start(context, agent)\n",
    "\n",
    "    def on_tool_start(\n",
    "            self,\n",
    "            context: RunContextWrapper[TContext],\n",
    "            agent: Agent[TContext],\n",
    "            tool: Tool) -> None:\n",
    "        print(f\"[on_tool_start] Agent: {agent.name}\")\n",
    "        print(f\"[on_tool_start] Tool: {tool.name}\")\n",
    "        print(f\"[on_tool_start] Context: {context}\")\n",
    "        return super().on_tool_start(context, agent, tool)\n",
    "\n",
    "    def def_on_tool_end(\n",
    "            self,\n",
    "            context: RunContextWrapper[TContext],\n",
    "            agent: Agent[TContext],\n",
    "            tool: Tool,\n",
    "            result: str) -> None:\n",
    "        print(f\"[on_tool_end] Agent: {agent.name}\")\n",
    "        print(f\"[on_tool_end] Tool: {tool.name}\")\n",
    "        print(f\"[on_tool_end] Context: {context}\")\n",
    "        print(f\"[on_tool_end] Result: {result}\")\n",
    "\n",
    "        return super().on_tool_end(context, agent, tool, result)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646b638",
   "metadata": {},
   "source": [
    "## Run the Handoff Workflow\n",
    "\n",
    "\n",
    "![handoff flow](./viz/07_agent_handoffs_orchestrator_agent.svg)\n",
    "\n",
    "This visualization shows the agent flow we've created, where the orchestrator agent directs requests to the appropriate specialized agents. The dad jokes generator produces jokes, which can then be handed off to language agents (Spanish or French) for translation, and finally to the user response agent to format the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "412fe1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace URL: https://platform.openai.com/traces/trace?trace_id=trace_64b300708c0e4c6ba1555154b69b18c7\n",
      "[on_agent_start] Agent: orchestrator_agent\n",
      "[on_agent_start] Context: RunContextWrapper(context=None, usage=Usage(requests=0, input_tokens=0, output_tokens=0, total_tokens=0))\n",
      "[handoff] Handoff context: RunContextWrapper(context=None, usage=Usage(requests=1, input_tokens=264, output_tokens=17, total_tokens=281))\n",
      "[handoff] Handoff from: orchestrator_agent\n",
      "[handoff] Handoff to: dad_jokes_generator_agent\n",
      "[handoff] HandoffsCount: 1\n",
      "[on_agent_start] Agent: dad_jokes_generator_agent\n",
      "[on_agent_start] Context: RunContextWrapper(context=None, usage=Usage(requests=1, input_tokens=264, output_tokens=17, total_tokens=281))\n",
      "[handoff] Handoff context: RunContextWrapper(context=None, usage=Usage(requests=2, input_tokens=584, output_tokens=32, total_tokens=616))\n",
      "[handoff] Handoff from: dad_jokes_generator_agent\n",
      "[handoff] Handoff to: orchestrator_agent\n",
      "[handoff] HandoffsCount: 1\n",
      "[on_agent_start] Agent: orchestrator_agent\n",
      "[on_agent_start] Context: RunContextWrapper(context=None, usage=Usage(requests=2, input_tokens=584, output_tokens=32, total_tokens=616))\n",
      "[handoff] Handoff context: RunContextWrapper(context=None, usage=Usage(requests=3, input_tokens=923, output_tokens=46, total_tokens=969))\n",
      "[handoff] Handoff from: orchestrator_agent\n",
      "[handoff] Handoff to: user_response_agent\n",
      "[handoff] HandoffsCount: 1\n",
      "[on_agent_start] Agent: user_response_agent\n",
      "[on_agent_start] Context: RunContextWrapper(context=None, usage=Usage(requests=3, input_tokens=923, output_tokens=46, total_tokens=969))\n",
      "[on_agent_end] Agent: user_response_agent\n",
      "[on_agent_end] Context: RunContextWrapper(context=None, usage=Usage(requests=4, input_tokens=1134, output_tokens=147, total_tokens=1281))\n",
      "[on_agent_end] Output: **User Query:** \"Hello, tell me a joke about chickens in Spanish.\"\n",
      "\n",
      "**Steps Taken to Produce the Final Output:**\n",
      "- Received the user query requesting a joke about chickens in Spanish.\n",
      "- Transferred the request to a joke generator for Spanish jokes.\n",
      "- Retrieved a suitable chicken joke in Spanish.\n",
      "- Formulated the final response to present the joke clearly to the user.\n",
      "\n",
      "**Final Output:** \n",
      "\"¿Por qué cruzó la gallina la carretera? ¡Para llegar al otro lado!\"\n",
      "**User Query:** \"Hello, tell me a joke about chickens in Spanish.\"\n",
      "\n",
      "**Steps Taken to Produce the Final Output:**\n",
      "- Received the user query requesting a joke about chickens in Spanish.\n",
      "- Transferred the request to a joke generator for Spanish jokes.\n",
      "- Retrieved a suitable chicken joke in Spanish.\n",
      "- Formulated the final response to present the joke clearly to the user.\n",
      "\n",
      "**Final Output:** \n",
      "\"¿Por qué cruzó la gallina la carretera? ¡Para llegar al otro lado!\"\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, RawResponsesStreamEvent, Runner, TResponseInputItem, trace\n",
    "\n",
    "msg = input (\"Enter your message: \") or \"Hello, tell me a joke about chickens in spanish\"\n",
    "\n",
    "inputs: list[TResponseInputItem] = [{\"content\": msg, \"role\": \"user\"}]\n",
    "\n",
    "result = None\n",
    "# keeping a state object to track the workflow state\n",
    "agentic_workflow_state_object = {}\n",
    "try:\n",
    "    with trace(workflow_name=\"handoffs\", group_id=\"mdsi\") as tr:\n",
    "        print(f\"Trace URL: {get_trace_url(tr)}\")\n",
    "        result = await Runner.run(\n",
    "            starting_agent=orchestrator_agent,\n",
    "            input=inputs,\n",
    "            run_config=RunConfig(\n",
    "                model=model_name,\n",
    "                model_provider=GITHUB_MODEL_PROVIDER,\n",
    "                model_settings=ModelSettings(\n",
    "                    temperature=0.5,\n",
    "                    max_tokens=5000,\n",
    "                ),\n",
    "                ),\n",
    "\n",
    "            max_turns=10,\n",
    "            hooks=JokeWorkflowRunHook(\n",
    "                custom_state_object=agentic_workflow_state_object,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(result.final_output)\n",
    "        # print(result.last_agent)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615af1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks = RunHooks()\n",
    "hooks.on_handoff()\n",
    "hooks.on_tool_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd6cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)\n",
    "\n",
    "print(get_trace_url(tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06900f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.raw_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e79c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.trace_util import display_agent_execution_steps\n",
    "\n",
    "\n",
    "display_agent_execution_steps(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9b94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracing_url = f\"https://platform.openai.com/traces/trace?trace_id={tr.trace_id}\"\n",
    "\n",
    "print(f\"Tracing URL: {tracing_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac50a878",
   "metadata": {},
   "source": [
    "## Visualise the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef9a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.extensions.visualization import draw_graph\n",
    "\n",
    "try:\n",
    "    draw_graph(orchestrator_agent, filename=\"viz/07_agent_handoffs.gv\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd1ad9",
   "metadata": {},
   "source": [
    "# another explains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "french_agent = Agent(\n",
    "    name=\"french_agent\",\n",
    "    instructions=\"You only speak French\",\n",
    ")\n",
    "\n",
    "spanish_agent = Agent(\n",
    "    name=\"spanish_agent\",\n",
    "    instructions=\"You only speak Spanish\",\n",
    ")\n",
    "\n",
    "english_agent = Agent(\n",
    "    name=\"english_agent\",\n",
    "    instructions=\"You only speak English\",\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"triage_agent\",\n",
    "    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n",
    "    handoffs=[french_agent, spanish_agent, english_agent],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa0dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.responses import ResponseContentPartDoneEvent, ResponseTextDeltaEvent\n",
    "\n",
    "from agents import Agent, RawResponsesStreamEvent, Runner, TResponseInputItem, trace\n",
    "import uuid\n",
    "\n",
    "conversation_id = str(uuid.uuid4().hex[:16])\n",
    "\n",
    "msg = input(\"Hi! We speak French, Spanish and English. How can I help? \")\n",
    "agent = triage_agent\n",
    "inputs: list[TResponseInputItem] = []\n",
    "inputs.append({\"content\": msg, \"role\": \"user\"})\n",
    "\n",
    "while True:\n",
    "    # Each conversation turn is a single trace. Normally, each input from the user would be an\n",
    "    # API request to your app, and you can wrap the request in a trace()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with trace(\"Routing example\", group_id=conversation_id):\n",
    "        result = Runner.run_streamed(\n",
    "            agent,\n",
    "            input=inputs,\n",
    "            run_config=RunConfig(model=model_name, model_provider=GITHUB_MODEL_PROVIDER),\n",
    "        )\n",
    "        async for event in result.stream_events():\n",
    "            if not isinstance(event, RawResponsesStreamEvent):\n",
    "                continue\n",
    "            data = event.data\n",
    "            if isinstance(data, ResponseTextDeltaEvent):\n",
    "                print(data.delta, end=\"\", flush=True)\n",
    "            elif isinstance(data, ResponseContentPartDoneEvent):\n",
    "                print(\"\\n\")\n",
    "\n",
    "    inputs = result.to_input_list()\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    agent = result.current_agent\n",
    "    user_msg = input(\"Enter a message: \")\n",
    "    if user_msg in (\"exit\", \"quit\", \"\"):\n",
    "        break\n",
    "    inputs.append({\"content\": user_msg, \"role\": \"user\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29104308",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    draw_graph(orchestrator_agent, filename=\"viz/07_agent_handoffs.gv\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
